# **Cross-Sell and Upsell Prediction Model**

## **ğŸ” Project Overview**

This project delivers a fully containerized, end-to-end machine learning solution to predict which products customers are most likely to buy next. It incorporates **Apache Airflow** for orchestration, **DBT** for transformations in **Azure Synapse**, and **Metabase** for interactive dashboards, all managed with modern tools like **Poetry** and **Make** to streamline development and testing.

### **Objective**

- Build a scalable, automated pipeline for ingestion, transformation, and predictions.
- Deploy actionable recommendations to improve cross-sell and upsell opportunities.
- Provide stakeholders with insights through an intuitive **Metabase dashboard**.

---

## **ğŸ“Œ Key Features**

1. **Modern Dependency Management**:
   - Use **Poetry** for package management and virtual environment handling.
2. **Scalable Pipeline**:
   - Orchestrate workflows with **Apache Airflow**, containerized for portability.
3. **Automated Testing**:
   - Use **Make** for automation and **pytest** for robust testing.
4. **Interactive Visualization**:
   - Deploy **Metabase** dashboards for exploring predictions and trends.

---

## **ğŸ—‚ï¸ Dataset**

### **Source**

[Instacart Market Basket Analysis Dataset](https://www.kaggle.com/c/instacart-market-basket-analysis/data), which includes:

- **Orders**: Historical purchase data.
- **Products**: Metadata like names, categories, and departments.
- **Users**: Anonymized customer IDs.
- **Order Products**: Links between orders and purchased items.

This dataset forms the foundation for predictive modeling and personalized recommendations.

---

## **ğŸ—ï¸ Architecture**

### **Pipeline Overview**

1. **Containerized Services**:
   - **Apache Airflow**: Orchestrates data pipelines and manages dependencies.
   - **PostgreSQL**: Stores metadata for Airflow.
   - **Metabase**: Provides business intelligence through dashboards.
2. **Data Transformation**:
   - **DBT**: Performs SQL-based transformations and creates feature-rich datasets in Azure Synapse.
3. **Feature Engineering & Model Training**:
   - Python scripts prepare data and train machine learning models (e.g., XGBoost).
4. **API Deployment**:
   - A Flask API serves recommendations for downstream integration.
5. **Visualization**:
   - **Metabase Dashboard** for real-time insights.

### **Tool Integration**

- **Poetry**: Handles package dependencies and creates isolated environments.
- **Make**: Automates workflows like testing and deployment.
- **pytest**: Ensures code quality with comprehensive test coverage.

---

## **ğŸ’» Technologies**

### **Core Services**

- **Apache Airflow**: Workflow orchestration.
- **PostgreSQL**: Backend for Airflow metadata.
- **DBT**: SQL-based transformations in Azure Synapse.
- **Metabase**: Business intelligence and dashboards.

### **Machine Learning**

- **Python**:
  - Data Manipulation: `pandas`, `numpy`
  - Modeling: `xgboost`, `scikit-learn`
  - Visualization: `matplotlib`, `seaborn`

### **Containerization and Management**

- **Docker**: Containerized Airflow, Metabase, PostgreSQL, and supporting services.
- **Poetry**: Simplifies dependency management and ensures reproducibility.

---

## **ğŸ“Š Results**

- **Prediction Accuracy**:
  - Precision@K: 85% (example metric).
  - Recall@K: 72% (example metric).
- **Example Recommendations**:
  | **Customer ID** | **Recommended Products** |
  |------------------|--------------------------------|
  | 1001 | Yogurt, Bananas, Granola |
  | 1002 | Avocados, Spinach, Almond Milk|

- **Dashboard Insights**:
  - Segment customers by their likelihood to purchase specific product categories.
  - Explore high-demand product trends and seasonal variations.

_(Add final metrics and dashboard screenshots once available.)_

---

## **ğŸš€ Development Workflow**

### **1. Clone the Repository**

```bash
git clone https://github.com/your-username/cross-sell-upsell-model.git
cd cross-sell-upsell-model
```

### **2. Install Dependencies**

Use **Poetry** to set up the environment and manage dependencies:

```bash
poetry install
poetry shell
```

### **3. Build and Deploy Containers**

Run all containerized services with Docker Compose:

```bash
docker-compose up --build
```

This will start:

- **Apache Airflow** (`http://localhost:8080`)
- **PostgreSQL** for Airflow metadata
- **Metabase** (`http://localhost:3000`)

### **4. Run Tests**

Use **Make** to run all tests with pytest:

```bash
make test
```

### **5. Execute Airflow DAGs**

1. Access the Airflow UI at `http://localhost:8080`.
2. Trigger the `cross_sell_dag` to run data ingestion, transformation, and prediction tasks.

### **6. Deploy DBT Models**

Run DBT transformations to prepare data in Azure Synapse:

```bash
dbt run
```

### **7. Visualize in Metabase**

Connect Metabase to Azure Synapse or PostgreSQL to explore predictions and trends.

---

## **ğŸ“ Folder Structure**

```
cross-sell-upsell-model/
â”‚
â”œâ”€â”€ dags/ # Airflow DAGs for orchestrating the pipeline
â”‚ â”œâ”€â”€ ingestion_dag.py
â”‚ â”œâ”€â”€ transformation_dag.py
â”‚ â”œâ”€â”€ model_training_dag.py
â”‚ â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ gcs_utils.py
â”‚ â”œâ”€â”€ bq_utils.py
â”‚
â”œâ”€â”€ terraform/ # Terraform configurations for GCP resources
â”‚ â”œâ”€â”€ main.tf # Main Terraform configuration
â”‚ â”œâ”€â”€ variables.tf # Input variables for Terraform
â”‚ â”œâ”€â”€ terraform.tfvars # Environment-specific variable values
â”‚ â”œâ”€â”€ outputs.tf # Output definitions
â”‚
â”œâ”€â”€ dbt/ # DBT project for BigQuery transformations
â”‚ â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ staging/
â”‚ â”œâ”€â”€ stg_orders.sql
â”‚ â”œâ”€â”€ stg_products.sql
â”‚ â”œâ”€â”€ marts/
â”‚ â”œâ”€â”€ user_features.sql
â”‚ â”œâ”€â”€ product_features.sql
â”‚ â”œâ”€â”€ dbt_project.yml # DBT project configuration
â”‚ â”œâ”€â”€ profiles.yml # DBT profiles for connecting to BigQuery
â”‚
â”œâ”€â”€ src/ # Source code for data processing and ML models
â”‚ â”œâ”€â”€ preprocessing.py # Data cleaning and preprocessing scripts
â”‚ â”œâ”€â”€ feature_engineering.py # Feature engineering logic
â”‚ â”œâ”€â”€ train_model.py # Model training script
â”‚ â”œâ”€â”€ serve_model.py # Flask API for serving predictions
â”‚ â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ logging_utils.py # Logging utilities
â”‚ â”œâ”€â”€ config.py # Centralized configuration management
â”‚
â”œâ”€â”€ tests/ # Tests for pipeline, models, and infrastructure
â”‚ â”œâ”€â”€ test_dags.py # Unit tests for Airflow DAGs
â”‚ â”œâ”€â”€ test_transformations.py # Unit tests for DBT models
â”‚ â”œâ”€â”€ test_model.py # Unit tests for ML models
â”‚
â”œâ”€â”€ docker/ # Docker configurations for containerized services
â”‚ â”œâ”€â”€ Dockerfile # Dockerfile for Flask API
â”‚ â”œâ”€â”€ docker-compose.yml # Docker Compose for local development
â”‚
â”œâ”€â”€ notebooks/ # Jupyter notebooks for exploration and prototyping
â”‚ â”œâ”€â”€ data_exploration.ipynb # Initial dataset exploration
â”‚ â”œâ”€â”€ feature_analysis.ipynb # Feature engineering analysis
â”‚ â”œâ”€â”€ model_training.ipynb # Model development and experimentation
â”‚
â”œâ”€â”€ metabase/ # Metabase configurations for dashboard setup
â”‚ â”œâ”€â”€ dashboard_config.json # Exported Metabase dashboard configuration
â”‚ â”œâ”€â”€ queries/
â”‚ â”œâ”€â”€ kpi_query.sql # SQL queries for key KPIs
â”‚
â”œâ”€â”€ ci_cd/ # CI/CD pipeline configurations
â”‚ â”œâ”€â”€ github_actions.yml # GitHub Actions workflow for CI/CD
â”‚ â”œâ”€â”€ cloudbuild.yaml # Cloud Build configuration
â”‚
â”œâ”€â”€ Makefile # Automation tasks (e.g., testing, deployment)
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ pyproject.toml # Poetry configuration for Python dependencies
â”œâ”€â”€ README.md # Project documentation
â”œâ”€â”€ .gitignore # Git ignore rules
â”œâ”€â”€ LICENSE # Project license
```

---

## **ğŸ“ˆ Future Enhancements**

1. **Real-Time Predictions**:
   - Add streaming pipelines with Kafka or Azure Event Hub for live inference.
2. **Advanced Modeling**:
   - Explore deep learning techniques for improved personalization.
3. **A/B Testing**:
   - Implement A/B testing to assess the impact of recommendations on sales.
4. **Data Enrichment**:
   - Integrate external data sources (e.g., customer demographics, marketing campaigns).

---

## **ğŸ“¬ Contact**

**Dr. Jody-Ann Jones**
ğŸ“§ Email: [jody@thedatasensei.com](mailto:jody@thedatasensei.com)
ğŸŒ Website: [The Data Sensei](https://www.thedatasensei.com)
ğŸ‘©â€ğŸ’¼ LinkedIn: [Dr. Jody-Ann Jones](https://www.linkedin.com/in/drjodyannjones)

---
